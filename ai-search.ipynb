{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2123c139-285e-40f7-bcf0-b1278359283a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Faker\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize Faker to generate random data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m fake \u001b[38;5;241m=\u001b[39m Faker()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker to generate random data\n",
    "fake = Faker()\n",
    "\n",
    "# Constants\n",
    "NUM_CLIENT_REQUESTS = 10000\n",
    "NUM_FREELANCERS = 1000\n",
    "INDUSTRIES = ['Technology', 'Marketing', 'Finance', 'Healthcare', 'Education']\n",
    "SKILLS = [\n",
    "    ['HTML', 'CSS', 'JavaScript', 'React'],\n",
    "    ['Python', 'Django', 'Flask'],\n",
    "    ['Java', 'Spring', 'Hibernate'],\n",
    "    ['Adobe Illustrator', 'Photoshop', 'UI/UX Design'],\n",
    "    ['SEO', 'Content Writing', 'Social Media Marketing'],\n",
    "]\n",
    "\n",
    "# Generate Freelancer Profiles\n",
    "freelancer_profiles = []\n",
    "for freelancer_id in range(1, NUM_FREELANCERS + 1):\n",
    "    profile = {\n",
    "        'Freelancer ID': freelancer_id,\n",
    "        'Name': fake.name(),\n",
    "        'Location': fake.city(),\n",
    "        'Industry': random.choice(INDUSTRIES),\n",
    "        'Skills': random.choice(SKILLS),\n",
    "        'Experience Level': random.choice(['Junior', 'Mid-Level', 'Senior']),\n",
    "        'Rating': round(random.uniform(3.0, 5.0), 1),\n",
    "        'Hourly Rate': random.randint(20, 150),\n",
    "    }\n",
    "    freelancer_profiles.append(profile)\n",
    "\n",
    "# Generate Client Requests\n",
    "client_requests = []\n",
    "for request_id in range(1, NUM_CLIENT_REQUESTS + 1):\n",
    "    industry = random.choice(INDUSTRIES)\n",
    "    request = {\n",
    "        'Request ID': request_id,\n",
    "        'Keywords': f\"{random.choice(['I need', 'Looking for', 'Require'])} {random.choice(['talented', 'experienced', 'creative'])} {random.choice(['developer', 'designer', 'writer', 'marketer'])}\",\n",
    "        'Industry': industry,\n",
    "        'Description': fake.sentence(nb_words=10),\n",
    "        'Budget': random.randint(100, 5000),\n",
    "        'Location': fake.city(),\n",
    "        'Skills Required': random.choice(SKILLS),\n",
    "        'Urgency': random.choice(['Immediate', 'Within a week', 'Flexible']),\n",
    "    }\n",
    "    client_requests.append(request)\n",
    "\n",
    "# Generate Search Queries\n",
    "search_queries = []\n",
    "for query_id in range(1, NUM_CLIENT_REQUESTS + 1):\n",
    "    search = {\n",
    "        'Query ID': query_id,\n",
    "        'Client ID': random.randint(1, NUM_CLIENT_REQUESTS),  # Random Client ID\n",
    "        'Search Terms': client_requests[query_id - 1]['Keywords'],\n",
    "        'Results Returned': ','.join([str(random.randint(1, NUM_FREELANCERS)) for _ in range(random.randint(1, 5))]),\n",
    "        'Timestamp': fake.date_time_this_year(),\n",
    "    }\n",
    "    search_queries.append(search)\n",
    "\n",
    "# Convert to DataFrame\n",
    "freelancer_df = pd.DataFrame(freelancer_profiles)\n",
    "client_requests_df = pd.DataFrame(client_requests)\n",
    "search_queries_df = pd.DataFrame(search_queries)\n",
    "\n",
    "# Save to CSV\n",
    "freelancer_df.to_csv('freelancer_profiles.csv', index=False)\n",
    "client_requests_df.to_csv('client_requests.csv', index=False)\n",
    "search_queries_df.to_csv('search_queries.csv', index=False)\n",
    "\n",
    "print(\"Dataset generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc481152-82ce-48cc-b9d9-dc1814eaff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/abhishek9773/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/abhishek9773/.local/lib/python3.10/site-packages (1.24.0)\n",
      "Requirement already satisfied: scikit-learn in /home/abhishek9773/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: tensorflow in /home/abhishek9773/.local/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /home/abhishek9773/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 KB\u001b[0m \u001b[31m720.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/abhishek9773/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/abhishek9773/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/abhishek9773/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: packaging in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: rich in /home/abhishek9773/.local/lib/python3.10/site-packages (from keras) (13.9.2)\n",
      "Requirement already satisfied: namex in /home/abhishek9773/.local/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/abhishek9773/.local/lib/python3.10/site-packages (from keras) (0.13.0)\n",
      "Collecting starlette<0.39.0,>=0.37.2\n",
      "  Downloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h11>=0.8 in /home/abhishek9773/.local/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abhishek9773/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from starlette<0.39.0,>=0.37.2->fastapi) (4.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/abhishek9773/.local/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/abhishek9773/.local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/abhishek9773/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Installing collected packages: uvicorn, pydantic-core, annotated-types, starlette, pydantic, fastapi\n",
      "Successfully installed annotated-types-0.7.0 fastapi-0.115.0 pydantic-2.9.2 pydantic-core-2.23.4 starlette-0.38.6 uvicorn-0.31.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install pandas numpy scikit-learn tensorflow keras fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ceec7-671d-4b95-9f53-a64be53527a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "freelancer_df = pd.read_csv('freelancer_profiles.csv')\n",
    "client_requests_df = pd.read_csv('client_requests.csv')\n",
    "search_queries_df = pd.read_csv('search_queries.csv')\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(freelancer_df.head())\n",
    "print(client_requests_df.head())\n",
    "print(search_queries_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6e0e3-16b7-467b-9bb4-c30cc20d5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Merging client requests with freelancer profiles for training\n",
    "merged_data = pd.merge(client_requests_df, freelancer_df, how='cross')\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_requests = vectorizer.fit_transform(merged_data['Description'])  # Description from client requests\n",
    "X_skills = vectorizer.transform(merged_data['Skills Required'])      # Skills from freelancer profiles\n",
    "\n",
    "# Combine features (you might want to use different methods to combine)\n",
    "X = np.hstack((X_requests.toarray(), X_skills.toarray()))\n",
    "\n",
    "# Target variable: Freelancer ID to predict\n",
    "y = merged_data['Freelancer ID'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7766b-9fd0-4986-921b-dc12f909d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(NUM_FREELANCERS, activation='softmax'))  # Output layer for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cd4d6-67ed-48a1-8284-b65f847c4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aaa50c-bca8-4735-98b8-08d4f5edd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26585c-185c-4c5b-82df-608d1d59d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api integration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622c614-0625-4514-8e6c-c66388f65f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model.save('freelancer_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb83196-783f-4643-8951-f57cc7bd4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Code Using FastAPI\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the pre-trained model and other necessary components\n",
    "model = tf.keras.models.load_model('freelancer_model.h5')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Assuming you saved the vectorizer\n",
    "\n",
    "# Example freelancer profiles (this would normally be fetched from a database)\n",
    "freelancer_df = pd.read_csv('freelancer_profiles.csv')\n",
    "\n",
    "# Define the input structure for the API\n",
    "class ClientRequest(BaseModel):\n",
    "    description: str\n",
    "    skills_required: list\n",
    "    budget: float\n",
    "    location: str\n",
    "\n",
    "# API Home route\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Freelancer Recommendation API\"}\n",
    "\n",
    "# Prediction route\n",
    "@app.post(\"/recommend_freelancers\")\n",
    "def recommend_freelancers(request: ClientRequest):\n",
    "    # Preprocess input (similar to what was done during training)\n",
    "    X_description = vectorizer.transform([request.description])\n",
    "    X_skills = vectorizer.transform([' '.join(request.skills_required)])  # Join skills into a string\n",
    "    X = np.hstack((X_description.toarray(), X_skills.toarray()))\n",
    "\n",
    "    # Make a prediction using the model\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Return top 3 freelancer IDs based on highest probabilities\n",
    "    top_freelancer_indices = np.argsort(predictions[0])[-3:]  # Get the top 3 predictions\n",
    "    top_freelancers = freelancer_df.iloc[top_freelancer_indices]\n",
    "\n",
    "    # If no freelancers are found, return a 404 error\n",
    "    if top_freelancers.empty:\n",
    "        raise HTTPException(status_code=404, detail=\"No matching freelancers found\")\n",
    "\n",
    "    # Construct the response\n",
    "    response = []\n",
    "    for _, freelancer in top_freelancers.iterrows():\n",
    "        response.append({\n",
    "            \"freelancer_id\": freelancer['Freelancer ID'],\n",
    "            \"name\": freelancer['Name'],\n",
    "            \"skills\": freelancer['Skills'],\n",
    "            \"hourly_rate\": freelancer['Hourly Rate'],\n",
    "            \"rating\": freelancer['Rating'],\n",
    "            \"location\": freelancer['Location']\n",
    "        })\n",
    "\n",
    "    return {\"recommended_freelancers\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a8b7a-2ae0-457b-bc63-68ad84d3c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the API\n",
    "uvicorn main:app --reload\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
